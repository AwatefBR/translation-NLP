{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6348334b",
   "metadata": {},
   "source": [
    " In this exercise, we will implement an encoder-decoder algorithm that allows for translation. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - TODO : Import and load the french and english sentence (2 .txt files)\n",
    "\n",
    "##You will store the English sentences in a variable named 'english_sentences' and the French ones in a variable named 'french_sentences'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629eba88-e902-417a-9523-dfd7073f8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_english.txt', 'r') as file:\n",
    "    english_sentences = file.readlines()\n",
    "\n",
    "with open('data_french.txt', 'r') as file:\n",
    "    french_sentences = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbd054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 - TODO : HOW MANY SENTENCES DO YOU HAVE ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56095556-51cc-4136-b935-486d955adf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137860, 137860)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_sentences), len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea0d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\\n\",\n",
       "  'les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\\n',\n",
       "  'california est généralement calme en mars , et il est généralement chaud en juin .\\n',\n",
       "  'les états-unis est parfois légère en juin , et il fait froid en septembre .\\n',\n",
       "  'votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\\n'],\n",
       " ['new jersey is sometimes quiet during autumn , and it is snowy in april .\\n',\n",
       "  'the united states is usually chilly during july , and it is usually freezing in november .\\n',\n",
       "  'california is usually quiet during march , and it is usually hot in june .\\n',\n",
       "  'the united states is sometimes mild during june , and it is cold in september .\\n',\n",
       "  'your least liked fruit is the grape , but my least liked is the apple .\\n'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3 - TODO : Display the first 5 sentences and their translation.\n",
    "french_sentences[:5], english_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfd29f7-88e2-4591-ae44-baf4c2e91144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9847063"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO : 4 - Count number of words in all english sentences\n",
    "sum_french = sum(len(elt) for elt in french_sentences)\n",
    "sum_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96eafebe-29f6-43a6-bedf-8e0e7115ade5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  4-2 - Count number of different unique words in all french sentences\n",
    "unique_words_fr = set()\n",
    "for sentence in french_sentences:\n",
    "    unique_words_fr.update(set(sentence.lower().split()))\n",
    "len(unique_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8e5cfd-15b5-4905-a253-7d31691b2fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est: 196809 times\n",
      ".: 135619 times\n",
      ",: 123135 times\n",
      "en: 105768 times\n",
      "il: 84079 times\n",
      "les: 65255 times\n",
      "mais: 63987 times\n",
      "et: 59851 times\n",
      "la: 49861 times\n",
      "parfois: 37746 times\n"
     ]
    }
   ],
   "source": [
    "##  4-3 - Display the 10 most frequent french word in the sentences\n",
    "from collections import Counter\n",
    "all_words_fr = []\n",
    "for sentence in french_sentences:\n",
    "    all_words_fr.extend(sentence.lower().split())\n",
    "word_counts_fr = Counter(all_words_fr)\n",
    "for word, count in word_counts_fr.most_common(10):\n",
    "    print(f\"{word}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca836d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9085266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO  5 - Do the same questions with english sentences\n",
    "## TODO : 5-1 - Count number of words in all english sentences\n",
    "sum_english = sum(len(elt) for elt in english_sentences)\n",
    "sum_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b893f1ca-7829-4c7c-94ba-28b2d96a843a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  TODO 5-2 - Count number of different unique words in all english sentences\n",
    "unique_words_en = set()\n",
    "for sentence in english_sentences:\n",
    "    unique_words_en.update(set(sentence.lower().split()))\n",
    "len(unique_words_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773225db-0b37-48d9-b8d8-49aed11ee66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is: 205858 times\n",
      ",: 140897 times\n",
      ".: 129039 times\n",
      "in: 75525 times\n",
      "it: 75137 times\n",
      "during: 74933 times\n",
      "the: 67628 times\n",
      "but: 63987 times\n",
      "and: 59850 times\n",
      "sometimes: 37746 times\n"
     ]
    }
   ],
   "source": [
    "##  5-3 - Display the 10 most frequent english word in the sentences\n",
    "all_words_en = []\n",
    "for sentence in english_sentences:\n",
    "    all_words_en.extend(sentence.lower().split())\n",
    "word_counts_en = Counter(all_words_en)\n",
    "for word, count in word_counts_en.most_common(10):\n",
    "    print(f\"{word}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e407e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:17:03.411332: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## TODO 6 - Create a function (named tokenize) that allows you to tokenize your sentences:\n",
    "## The function return list of tokenized sentences and the tokenizer\n",
    "##Indeed, the network only takes sequences of numbers!\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def tokenize(text) :\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    text_token = tokenizer.texts_to_sequences(text)\n",
    "    return text_token, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32125ebd-4fb9-42a5-b2e0-8f1e59098bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize(french_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce3ea4a-e3fd-4cb8-b1cc-b5c6e7f35cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize(english_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1ecb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7 - Create a function (named pad) that allows you to pad your data. It will take two arguments: 'x,' which is a list of sequences, and 'length,' which is the maximum length we desire. \n",
    "##By default, 'length' will be set to None, meaning padding with the max of sequence length\n",
    "## PS : you can use the sequence.pad_sequences function\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "def pad(x, length=None):\n",
    "    pad_data = sequence.pad_sequences(x, maxlen = length,padding = 'post')\n",
    "    return pad_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593419d-ea64-465b-ad95-96c4630fe26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b27a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8-  Use this function (and understand it) and apply on french and english sentence\n",
    "\n",
    "def preprocess(x,y):\n",
    "    \n",
    "    \"\"\"\"\n",
    "     x : list de phrases à preprocesser\n",
    "     y : list de phrases à preprocesser\n",
    "     return : np array de phrases preprocesser , et leur tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess_x , x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x,length = None)\n",
    "    preprocess_y = pad(preprocess_y,length = None)\n",
    "    \n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    \n",
    "    return preprocess_x,preprocess_y,x_tk,y_tk\n",
    "\n",
    "preproc_english, preproc_french , english_tokenizer , french_tokenizer =  preprocess(english_sentences,french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad765362-0a01-462a-8740-c5eb0a43a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess(english_sentences,french_sentences)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c058f9d0-f555-44cf-8e34-5a387c3236b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e80a767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "## 9 - Apply this function \n",
    "\n",
    "import numpy as np\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d276ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11 - We are going to translate english sentences to french sentences \n",
    "### Create a Sequential model with : * Embedding layer (input_dim = number of word in english dictionnary,\n",
    "#                                                       output_dim = 256 (embedding size),\n",
    "#                                                       input_length = size max of the sequence (21))\n",
    "#.                                   * One LSTM Layer with 256 neurons (dont forget return_sequence = True)\n",
    "#                                    * Dropout(a) with a = 0.5\n",
    "#                                    * Dense(b) with b = french_vocab_size (number of words in french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4552ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 23,  1, ...,  0,  0,  0],\n",
       "       [ 5, 20, 21, ...,  0,  0,  0],\n",
       "       [22,  1,  9, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [19,  1, 10, ...,  0,  0,  0],\n",
       "       [24,  1, 10, ...,  0,  0,  0],\n",
       "       [ 5, 84,  1, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 12 - Reshape the input with this line \n",
    "tmp_x = pad(preproc_english, preproc_french.shape[1])\n",
    "tmp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79927527-b72e-41bc-9b59-7f5e2ac531c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de9f994f-dd93-487c-bdf7-540b331defbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = len(english_tokenizer.index_word) + 1, output_dim = 256 , input_length = 21))\n",
    "    model.add(LSTM(units = 256, return_sequences = True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = len(french_tokenizer.index_word) + 1, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "337b0120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 21, 256)           525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21, 256)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21, 345)           88665     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 665177 (2.54 MB)\n",
      "Trainable params: 665177 (2.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##13 - Do model.summary() and find the number of estimated paramters using your own resources\n",
    "model = model_LSTM()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04a95588-808b-45f8-aadb-0b9224748476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525312"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(256+256)*256*4 +(256*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c7366a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3447/3447 [==============================] - 590s 170ms/step - loss: 0.7268 - accuracy: 0.8101 - val_loss: 0.2910 - val_accuracy: 0.9075\n",
      "Epoch 2/2\n",
      "3447/3447 [==============================] - 612s 177ms/step - loss: 0.3031 - accuracy: 0.9057 - val_loss: 0.2357 - val_accuracy: 0.9223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x12b785d20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 14 - Apply your model and print the original sentence, the result of the model and the desired output (true translation)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer = Adam(), loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.fit(tmp_x, preproc_french, validation_split = 0.2, batch_size = 32, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882f8d0-37e2-4b3b-873d-e454b3b55b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, accuracy = model.evaluate(tmp_x, preproc_french, verbose = 0)\n",
    "#loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "403c0e46-f4f2-4d61-980f-6f38c1d0a30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0, 17, 23,  1,  8, 67,  4, 39,  7,  3,\n",
       "        1, 55,  2, 44], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tmp_x[:1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3699864-4505-4f15-9f33-0fb35ccf5312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((tmp_x[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e3c89f2-8db3-45ee-9bfc-d61f0c8564f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d382837-5dae-4457-a9e6-d056528bf2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "960f5d1b-9895-4348-bd13-ed1f8233be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "1/1 [==============================] - 1s 803ms/step\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "\n",
      "Original text:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\")\n",
    "print(logits_to_text(model.predict(tmp_x[:1])[0],french_tokenizer))\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[0])\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b5d1f7c-8815-445e-a22d-5742402be819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "les états unis est parfois doux en juin et il est froid en septembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "\n",
      "\n",
      "Original text:\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\")\n",
    "print(logits_to_text(model.predict(tmp_x[3:4])[0], french_tokenizer))\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[3])\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f52f4d-6934-4d20-a501-759a51505890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
